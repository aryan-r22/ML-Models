{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Paraphraser.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNf8233SkdYKZWzmqn0PZWs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryan-r22/ML-Models/blob/master/Paraphraser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaCaTwzAcv69",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9900179a-9b56-4585-9bb2-069a1a2df9a3"
      },
      "source": [
        "!pip install torch==1.4.0\n",
        "!pip install transformers==2.9.0\n",
        "!pip install pytorch_lightning==0.7.5"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 18kB/s \n",
            "\u001b[31mERROR: torchvision 0.7.0+cu101 has requirement torch==1.6.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "Successfully installed torch-1.4.0\n",
            "Collecting transformers==2.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (2019.12.20)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 22.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 37.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.0) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=77dc93f68c2c42f7a3eab545ca331089978e1942e8e806e45af3a579f573499c\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.9.0\n",
            "Collecting pytorch_lightning==0.7.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ac/ac03f1f3fa950d96ca52f07d33fdbf5add05f164c1ac4eae179231dfa93d/pytorch_lightning-0.7.5-py3-none-any.whl (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning==0.7.5) (1.18.5)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning==0.7.5) (1.4.0)\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning==0.7.5) (2.3.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning==0.7.5) (4.41.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.31.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (0.4.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (0.35.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.15.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (1.7.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (49.6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (0.8.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.7.5) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.7.5) (1.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (2.10)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (4.6)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning==0.7.5) (0.4.8)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=394f6a9513dedf463bef1a7e977a9be4df9881eb4f13f1304054e01989f062bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built future\n",
            "Installing collected packages: future, pytorch-lightning\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed future-0.18.2 pytorch-lightning-0.7.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "900q0OxqdqVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import requests, zipfile, io\n",
        "import os\n",
        "\n",
        "zip_file_url = \"https://datascience-models-ramsri.s3.amazonaws.com/t5_paraphraser.zip\"\n",
        "folder_path = zip_file_url.split(\"/\")[-1].replace(\".zip\",\"\")\n",
        "\n",
        "if not os.path.exists(folder_path):\n",
        "    r = requests.get(zip_file_url)\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    z.extractall('./t5_paraphraser')\n",
        "else:\n",
        "    print (\"Folder available: \",folder_path)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBJ4lu3Wd3gW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "82b77e4f-4532-46e9-eccc-feef98ea8803"
      },
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained('./t5_paraphraser')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (\"device \",device)\n",
        "model = model.to(device)\n",
        "\n",
        "sentence = \"The car is running out of petrol\"\n",
        "# sentence = \"What are the ingredients required to bake a perfect cake?\"\n",
        "# sentence = \"What is the best possible approach to learn aeronautical engineering?\"\n",
        "# sentence = \"Do apples taste better than oranges in general?\"\n",
        "\n",
        "\n",
        "text =  \"paraphrase: \" + sentence + \" </s>\"\n",
        "\n",
        "\n",
        "max_len = 256\n",
        "\n",
        "encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
        "input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "\n",
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "beam_outputs = model.generate(\n",
        "    input_ids=input_ids, attention_mask=attention_masks,\n",
        "    do_sample=True,\n",
        "    max_length=256,\n",
        "    top_k=120,\n",
        "    top_p=0.98,\n",
        "    early_stopping=True,\n",
        "    num_return_sequences=10\n",
        ")\n",
        "\n",
        "\n",
        "print (\"\\nOriginal Question ::\")\n",
        "print (sentence)\n",
        "print (\"\\n\")\n",
        "print (\"Paraphrased Questions :: \")\n",
        "final_outputs =[]\n",
        "for beam_output in beam_outputs:\n",
        "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "    if sent.lower() != sentence.lower() and sent not in final_outputs:\n",
        "        final_outputs.append(sent)\n",
        "\n",
        "for i, final_output in enumerate(final_outputs):\n",
        "    print(\"{}: {}\".format(i, final_output))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device  cpu\n",
            "\n",
            "Original Question ::\n",
            "The car is running out of petrol\n",
            "\n",
            "\n",
            "Paraphrased Questions :: \n",
            "0: When my car runs out of petrol, I thought I might try to fix it myself.\n",
            "1: Car running out of petrol (Giver diesel). The car needs fuel when its run low.\n",
            "2: Car running out of fuel at the pump. The car is in a 0 point petrol station.\n",
            "3: Car is running out of petrol! We are in a diesel car which doesn't run petrol.\n",
            "4: Car running out of petrol. Why?\n",
            "5: Is my car running out of petrol?\n",
            "6: Car running out of petrol, fuel it needs.\n",
            "7: I run out of petrol in a car. I'm doing a cllc repair.\n",
            "8: If my car runs out of petrol (ie car batteries or diesel pump), would it be worth changing its engine to petrol instead?\n",
            "9: Why is my car running out of petrol?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqZxc0DYd5CY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kchBoMzaeo9Z",
        "colab_type": "text"
      },
      "source": [
        "Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l39TH2let5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = \"quora_duplicate_questions.tsv\"\n",
        "import pandas as pd\n",
        "question_pairs = pd.read_csv(filename, sep='\\t')\n",
        "question_pairs.drop(['qid1', 'qid2'], axis = 1,inplace = True)\n",
        "question_pairs_correct_paraphrased = question_pairs[question_pairs['is_duplicate']==1]\n",
        "question_pairs_correct_paraphrased.drop(['id', 'is_duplicate'], axis = 1,inplace = True)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(question_pairs_correct_paraphrased, test_size=0.1)\n",
        "train.to_csv('Quora_Paraphrasing_train.csv', index = False)\n",
        "test.to_csv('Quora_Paraphrasing_val.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no5AEz66ex7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ParaphraseDataset(Dataset):\n",
        "    def __init__(self, tokenizer, data_dir, type_path, max_len=256):\n",
        "        self.path = os.path.join(data_dir, type_path + '.csv')\n",
        "\n",
        "        self.source_column = \"question1\"\n",
        "        self.target_column = \"question2\"\n",
        "        self.data = pd.read_csv(self.path)\n",
        "\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
        "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
        "\n",
        "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
        "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
        "\n",
        "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
        "\n",
        "    def _build(self):\n",
        "        for idx in range(len(self.data)):\n",
        "            input_, target = self.data.loc[idx, self.source_column], self.data.loc[idx, self.target_column]\n",
        "\n",
        "            input_ = \"paraphrase: \"+ input_ + ' </s>'\n",
        "            target = target + \" </s>\"\n",
        "\n",
        "            # tokenize inputs\n",
        "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "                [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
        "            )\n",
        "            # tokenize targets\n",
        "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "                [target], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            self.inputs.append(tokenized_inputs)\n",
        "            self.targets.append(tokenized_targets)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}